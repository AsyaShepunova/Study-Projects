{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for Wikishop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikishop online store launches a new service. Now users can edit and supplement product descriptions, as in wiki communities. That is, clients offer their edits and comment on the changes of others. The store needs a tool that will search for toxic comments and send them for moderation.\n",
    "\n",
    "In this project, we will train a machine learning model to classify comments into positive and negative. Our goal will be to reach *F1* at least 0.75.\n",
    "\n",
    "\n",
    "**Data description**\n",
    "\n",
    "The *text* column contains the comment text, and *toxic* is the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import optuna\n",
    "import pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#algorithms which we are going try\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                text  toxic\n",
       " 0  Explanation\\nWhy the edits made under my usern...      0\n",
       " 1  D'aww! He matches this background colour I'm s...      0\n",
       " 2  Hey man, I'm really not trying to edit war. It...      0\n",
       " 3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       " 4  You, sir, are my hero. Any chance you remember...      0,\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('/Users/Shepunova/Desktop/Data_Science/Projects/Data/toxic_comments.csv')\n",
    "data_raw.head(), data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shepunova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shepunova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shepunova/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function which will remove unnecessary words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "data_raw['text'] = data_raw['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a function for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z']\", ' ', text) #leave only the letters a-z\n",
    "    text = text.split() #split the text into separate words\n",
    "    text = \" \".join(text) #combine the words into a string separated by a space\n",
    "    lemm_list = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    lemm_text = \" \".join(lemm_list)\n",
    "    return lemm_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 159 thousand rows in the data. To select the best model, we will choose random 50 thousand texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose random 50 thousand texts\n",
    "data_model_selection = data_raw.sample(50000).reset_index(drop=True)\n",
    "data_model_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 37s, sys: 1min 15s, total: 7min 52s\n",
      "Wall time: 8min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#lemmatization\n",
    "data_model_selection['lemmas'] = data_model_selection['text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yep hopefully i am going to take a look at tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>yep hopefully i be go to take a look at that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best regards wolfowitz</td>\n",
       "      <td>0</td>\n",
       "      <td>best regard wolfowitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am new at this and may make a few mistakes b...</td>\n",
       "      <td>0</td>\n",
       "      <td>i be new at this and may make a few mistake bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know you are a lord i am sorry sir c...</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not know you be a lord i be sorry sir con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woah as someone who would been the victim of h...</td>\n",
       "      <td>0</td>\n",
       "      <td>woah a someone who would be the victim of his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  yep hopefully i am going to take a look at tha...      0   \n",
       "1                             best regards wolfowitz      0   \n",
       "2  i am new at this and may make a few mistakes b...      0   \n",
       "3  i did not know you are a lord i am sorry sir c...      0   \n",
       "4  woah as someone who would been the victim of h...      0   \n",
       "\n",
       "                                              lemmas  \n",
       "0  yep hopefully i be go to take a look at that a...  \n",
       "1                              best regard wolfowitz  \n",
       "2  i be new at this and may make a few mistake bu...  \n",
       "3  i do not know you be a lord i be sorry sir con...  \n",
       "4  woah a someone who would be the victim of his ...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at the lemmatization results\n",
    "data_model_selection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with the calssification problem, let's check the class balance first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    143346\n",
      "1     16225\n",
      "Name: toxic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.834884437596301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_raw['toxic'].value_counts())\n",
    "class_ratio = data_raw['toxic'].value_counts()[0]/data_raw['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are not balanced. The ratio is 1:8.83. We will try to change the weights in the training model.\n",
    "Let's prepare features and target before the test training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into features and target\n",
    "X = data_model_selection['lemmas']\n",
    "y = data_model_selection['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (37500,), (12500,), (12500,))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#divide features and target into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Вычислим tf-idf для текстов. Исключив стоп слова\n",
    "#stopwords = set(nltk_stopwords.words('english'))\n",
    "#создаем счетчик\n",
    "#count = TfidfVectorizer(stop_words = stopwords, min_df = 10)\n",
    "#count.fit_transform(X_train)\n",
    "#tf_idf_train = count.transform(X_train)\n",
    "#tf_idf_test = count.transform(X_test)\n",
    "#Посмотрим размер получившейся матрицы\n",
    "#tf_idf_train.shape, tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have selected 50 thousand random twits, lemmatized them and splitted into test and train sets.\n",
    "We now have 37500 observations in the train set and 12500 observations in the test set. Each row has 7393 features.\n",
    "Let's start testing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Default Models\n",
    "We will first test several models with default hyperparameters and choose the best time and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for cross validation\n",
    "cv_outer=StratifiedKFold(n_splits=5, random_state=12345, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will test several algorythms at once\n",
    "#As an argument, function takes a DataFrame with ML models \n",
    "#It returns DataFrame with f1 score and calculated time for each model \n",
    "\n",
    "def test(models): \n",
    "    results =  {} \n",
    "    for i in models:\n",
    "        start_time = time.time()\n",
    "        pipe = Pipeline([\n",
    "        ('prep', TfidfVectorizer(stop_words=stopwords, min_df = 10)),\n",
    "        ('est', models[i])]) #Prepare data with word vectotrizer using pipeline\n",
    "        \n",
    "        #calculate f1 score for each model\n",
    "        f1 = cross_val_score(pipe,  X_train, y_train, scoring = 'f1', cv = cv_outer).mean() \n",
    "        elapsed_time = time.time() - start_time #calculate time for training model\n",
    "        results[i] = [f1, elapsed_time]\n",
    "    return pd.DataFrame(results, index = ['F1', 'Time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(random_state=12345),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state = 12345),\n",
    "         'Random Forest': RandomForestClassifier(random_state = 12345),\n",
    "          'Catboost': CatBoostClassifier(verbose=50),\n",
    "         'K-neighbours': KNeighborsClassifier(n_neighbors = 1),\n",
    "         'Linear SVC': LinearSVC(random_state = 12345)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.044021\n",
      "0:\tlearn: 0.6492746\ttotal: 177ms\tremaining: 2m 56s\n",
      "50:\tlearn: 0.2229225\ttotal: 6.91s\tremaining: 2m 8s\n",
      "100:\tlearn: 0.1955475\ttotal: 13.6s\tremaining: 2m 1s\n",
      "150:\tlearn: 0.1809679\ttotal: 20.4s\tremaining: 1m 54s\n",
      "200:\tlearn: 0.1704951\ttotal: 27.4s\tremaining: 1m 48s\n",
      "250:\tlearn: 0.1625447\ttotal: 34.3s\tremaining: 1m 42s\n",
      "300:\tlearn: 0.1555973\ttotal: 41.3s\tremaining: 1m 36s\n",
      "350:\tlearn: 0.1492229\ttotal: 48.1s\tremaining: 1m 28s\n",
      "400:\tlearn: 0.1441965\ttotal: 55.5s\tremaining: 1m 22s\n",
      "450:\tlearn: 0.1397705\ttotal: 1m 3s\tremaining: 1m 17s\n",
      "500:\tlearn: 0.1353440\ttotal: 1m 10s\tremaining: 1m 10s\n",
      "550:\tlearn: 0.1312206\ttotal: 1m 17s\tremaining: 1m 3s\n",
      "600:\tlearn: 0.1276012\ttotal: 1m 24s\tremaining: 56.2s\n",
      "650:\tlearn: 0.1242495\ttotal: 1m 32s\tremaining: 49.3s\n",
      "700:\tlearn: 0.1210165\ttotal: 1m 40s\tremaining: 42.8s\n",
      "750:\tlearn: 0.1184079\ttotal: 1m 48s\tremaining: 35.9s\n",
      "800:\tlearn: 0.1165515\ttotal: 1m 56s\tremaining: 28.9s\n",
      "850:\tlearn: 0.1147529\ttotal: 2m 3s\tremaining: 21.7s\n",
      "900:\tlearn: 0.1130273\ttotal: 2m 11s\tremaining: 14.5s\n",
      "950:\tlearn: 0.1113683\ttotal: 2m 19s\tremaining: 7.21s\n",
      "999:\tlearn: 0.1097979\ttotal: 2m 27s\tremaining: 0us\n",
      "Learning rate set to 0.044021\n",
      "0:\tlearn: 0.6476321\ttotal: 186ms\tremaining: 3m 5s\n",
      "50:\tlearn: 0.2217569\ttotal: 7.9s\tremaining: 2m 27s\n",
      "100:\tlearn: 0.1945996\ttotal: 15.9s\tremaining: 2m 21s\n",
      "150:\tlearn: 0.1796966\ttotal: 24s\tremaining: 2m 14s\n",
      "200:\tlearn: 0.1693063\ttotal: 31.6s\tremaining: 2m 5s\n",
      "250:\tlearn: 0.1610182\ttotal: 39s\tremaining: 1m 56s\n",
      "300:\tlearn: 0.1540025\ttotal: 46.5s\tremaining: 1m 48s\n",
      "350:\tlearn: 0.1475091\ttotal: 54.2s\tremaining: 1m 40s\n",
      "400:\tlearn: 0.1419882\ttotal: 1m 1s\tremaining: 1m 32s\n",
      "450:\tlearn: 0.1377131\ttotal: 1m 9s\tremaining: 1m 24s\n",
      "500:\tlearn: 0.1335003\ttotal: 1m 17s\tremaining: 1m 17s\n",
      "550:\tlearn: 0.1296354\ttotal: 1m 24s\tremaining: 1m 9s\n",
      "600:\tlearn: 0.1260699\ttotal: 1m 32s\tremaining: 1m 1s\n",
      "650:\tlearn: 0.1226665\ttotal: 1m 40s\tremaining: 53.6s\n",
      "700:\tlearn: 0.1197165\ttotal: 1m 46s\tremaining: 45.5s\n",
      "750:\tlearn: 0.1170817\ttotal: 1m 53s\tremaining: 37.5s\n",
      "800:\tlearn: 0.1151644\ttotal: 2m\tremaining: 29.8s\n",
      "850:\tlearn: 0.1133398\ttotal: 2m 7s\tremaining: 22.2s\n",
      "900:\tlearn: 0.1116943\ttotal: 2m 13s\tremaining: 14.7s\n",
      "950:\tlearn: 0.1098246\ttotal: 2m 20s\tremaining: 7.22s\n",
      "999:\tlearn: 0.1082677\ttotal: 2m 26s\tremaining: 0us\n",
      "Learning rate set to 0.044021\n",
      "0:\tlearn: 0.6472466\ttotal: 164ms\tremaining: 2m 44s\n",
      "50:\tlearn: 0.2226413\ttotal: 6.72s\tremaining: 2m 5s\n",
      "100:\tlearn: 0.1957399\ttotal: 13.4s\tremaining: 1m 58s\n",
      "150:\tlearn: 0.1807970\ttotal: 19.9s\tremaining: 1m 51s\n",
      "200:\tlearn: 0.1702506\ttotal: 26.4s\tremaining: 1m 44s\n",
      "250:\tlearn: 0.1623366\ttotal: 33s\tremaining: 1m 38s\n",
      "300:\tlearn: 0.1551303\ttotal: 39.7s\tremaining: 1m 32s\n",
      "350:\tlearn: 0.1488450\ttotal: 46.6s\tremaining: 1m 26s\n",
      "400:\tlearn: 0.1433493\ttotal: 53.3s\tremaining: 1m 19s\n",
      "450:\tlearn: 0.1388582\ttotal: 59.7s\tremaining: 1m 12s\n",
      "500:\tlearn: 0.1344076\ttotal: 1m 6s\tremaining: 1m 5s\n",
      "550:\tlearn: 0.1305769\ttotal: 1m 13s\tremaining: 59.5s\n",
      "600:\tlearn: 0.1271917\ttotal: 1m 19s\tremaining: 52.9s\n",
      "650:\tlearn: 0.1238829\ttotal: 1m 26s\tremaining: 46.2s\n",
      "700:\tlearn: 0.1208700\ttotal: 1m 32s\tremaining: 39.6s\n",
      "750:\tlearn: 0.1180015\ttotal: 1m 39s\tremaining: 33s\n",
      "800:\tlearn: 0.1155888\ttotal: 1m 46s\tremaining: 26.5s\n",
      "850:\tlearn: 0.1138506\ttotal: 1m 53s\tremaining: 19.8s\n",
      "900:\tlearn: 0.1121069\ttotal: 1m 59s\tremaining: 13.1s\n",
      "950:\tlearn: 0.1103272\ttotal: 2m 6s\tremaining: 6.51s\n",
      "999:\tlearn: 0.1086251\ttotal: 2m 13s\tremaining: 0us\n",
      "Learning rate set to 0.044021\n",
      "0:\tlearn: 0.6482944\ttotal: 168ms\tremaining: 2m 47s\n",
      "50:\tlearn: 0.2234967\ttotal: 6.86s\tremaining: 2m 7s\n",
      "100:\tlearn: 0.1960029\ttotal: 13.4s\tremaining: 1m 59s\n",
      "150:\tlearn: 0.1813779\ttotal: 20.1s\tremaining: 1m 53s\n",
      "200:\tlearn: 0.1713371\ttotal: 26.8s\tremaining: 1m 46s\n",
      "250:\tlearn: 0.1632684\ttotal: 33.4s\tremaining: 1m 39s\n",
      "300:\tlearn: 0.1561235\ttotal: 40s\tremaining: 1m 32s\n",
      "350:\tlearn: 0.1496943\ttotal: 46.7s\tremaining: 1m 26s\n",
      "400:\tlearn: 0.1443127\ttotal: 53.2s\tremaining: 1m 19s\n",
      "450:\tlearn: 0.1399506\ttotal: 59.7s\tremaining: 1m 12s\n",
      "500:\tlearn: 0.1356154\ttotal: 1m 6s\tremaining: 1m 6s\n",
      "550:\tlearn: 0.1319755\ttotal: 1m 13s\tremaining: 59.7s\n",
      "600:\tlearn: 0.1283651\ttotal: 1m 19s\tremaining: 53.1s\n",
      "650:\tlearn: 0.1251906\ttotal: 1m 26s\tremaining: 46.6s\n",
      "700:\tlearn: 0.1218740\ttotal: 1m 33s\tremaining: 40s\n",
      "750:\tlearn: 0.1191843\ttotal: 1m 40s\tremaining: 33.3s\n",
      "800:\tlearn: 0.1170942\ttotal: 1m 47s\tremaining: 26.6s\n",
      "850:\tlearn: 0.1151637\ttotal: 1m 53s\tremaining: 20s\n",
      "900:\tlearn: 0.1135719\ttotal: 2m\tremaining: 13.3s\n",
      "950:\tlearn: 0.1118073\ttotal: 2m 7s\tremaining: 6.56s\n",
      "999:\tlearn: 0.1100399\ttotal: 2m 13s\tremaining: 0us\n",
      "Learning rate set to 0.044021\n",
      "0:\tlearn: 0.6492197\ttotal: 171ms\tremaining: 2m 50s\n",
      "50:\tlearn: 0.2250470\ttotal: 6.99s\tremaining: 2m 10s\n",
      "100:\tlearn: 0.1968405\ttotal: 13.9s\tremaining: 2m 3s\n",
      "150:\tlearn: 0.1822658\ttotal: 20.6s\tremaining: 1m 55s\n",
      "200:\tlearn: 0.1720268\ttotal: 27.3s\tremaining: 1m 48s\n",
      "250:\tlearn: 0.1637773\ttotal: 34s\tremaining: 1m 41s\n",
      "300:\tlearn: 0.1563545\ttotal: 40.9s\tremaining: 1m 34s\n",
      "350:\tlearn: 0.1498457\ttotal: 47.5s\tremaining: 1m 27s\n",
      "400:\tlearn: 0.1446075\ttotal: 54.2s\tremaining: 1m 21s\n",
      "450:\tlearn: 0.1399821\ttotal: 1m\tremaining: 1m 14s\n",
      "500:\tlearn: 0.1356465\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "550:\tlearn: 0.1315731\ttotal: 1m 14s\tremaining: 1m\n",
      "600:\tlearn: 0.1278287\ttotal: 1m 21s\tremaining: 54s\n",
      "650:\tlearn: 0.1244749\ttotal: 1m 27s\tremaining: 47.1s\n",
      "700:\tlearn: 0.1215627\ttotal: 1m 34s\tremaining: 40.3s\n",
      "750:\tlearn: 0.1188629\ttotal: 1m 41s\tremaining: 33.5s\n",
      "800:\tlearn: 0.1167030\ttotal: 1m 47s\tremaining: 26.8s\n",
      "850:\tlearn: 0.1144256\ttotal: 1m 54s\tremaining: 20s\n",
      "900:\tlearn: 0.1127913\ttotal: 2m 1s\tremaining: 13.3s\n",
      "950:\tlearn: 0.1110661\ttotal: 2m 7s\tremaining: 6.59s\n",
      "999:\tlearn: 0.1094251\ttotal: 2m 14s\tremaining: 0us\n",
      "CPU times: user 1h 17min 14s, sys: 52 s, total: 1h 18min 6s\n",
      "Wall time: 18min 38s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Catboost</th>\n",
       "      <th>K-neighbours</th>\n",
       "      <th>Linear SVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.655776</td>\n",
       "      <td>0.686154</td>\n",
       "      <td>0.733199</td>\n",
       "      <td>0.714337</td>\n",
       "      <td>0.374152</td>\n",
       "      <td>0.741894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>10.699842</td>\n",
       "      <td>190.428930</td>\n",
       "      <td>160.378992</td>\n",
       "      <td>708.013116</td>\n",
       "      <td>40.339485</td>\n",
       "      <td>9.076745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Logistic Regression  Decision Tree  Random Forest    Catboost  \\\n",
       "F1               0.655776       0.686154       0.733199    0.714337   \n",
       "Time            10.699842     190.428930     160.378992  708.013116   \n",
       "\n",
       "      K-neighbours  Linear SVC  \n",
       "F1        0.374152    0.741894  \n",
       "Time     40.339485    9.076745  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат по метрике даёт линейный SVC - 0.7418 \n",
    "<br>При этом он выполняет код быстрее всех остальных моделей. \n",
    "<br>На втором месте Random Forest с результатом 0.7331. Однако вреня выполнения в 17 раз выше, чем у Linear SVC\n",
    "<br>Попробуем оптимизировать Linear SVC настройкой гиперпарамертров\n",
    "\n",
    "**Linear SVC** preforms the best f1 score  - l 0.7418.\n",
    "<br>At the same time, it runs the code faster than all other models.\n",
    "<br>On the second place Random Forest with f1 score = 0.7331. However, it takes 17 times longer than Linear SVC. \n",
    "<<br>Let's try to optimize Linear SVC by setting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class_weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes={0:1, 1:class_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 244 ms, total: 11.9 s\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7415313212207305"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe = Pipeline([\n",
    "        ('prep', TfidfVectorizer(stop_words=stopwords)),\n",
    "        ('est', LinearSVC(class_weight = dict_classes, random_state=12345))])       \n",
    "    \n",
    "f1 = cross_val_score(pipe, X_train, y_train, cv=cv_outer, scoring='f1').mean()\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference. Let's try to set the parameter \"class_weight\" to balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 259 ms, total: 11.2 s\n",
      "Wall time: 11.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.744089285080044"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe = Pipeline([\n",
    "        ('prep', TfidfVectorizer(stop_words=stopwords)),\n",
    "        ('est', LinearSVC(class_weight = 'balanced', random_state=12345))])       \n",
    "    \n",
    "f1 = cross_val_score(pipe, X_train, y_train, cv=cv_outer, scoring='f1').mean()\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric has improved by 0.0022. Let's try to choose the parameters using the Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-29 13:48:58,150]\u001b[0m A new study created in memory with name: Linear Support Vector Classification Optuna\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:49:11,137]\u001b[0m Trial 0 finished with value: 0.7295617913765453 and parameters: {'max_iter': 2100, 'dual': True}. Best is trial 0 with value: 0.7295617913765453.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:49:23,556]\u001b[0m Trial 1 finished with value: 0.758451449241466 and parameters: {'max_iter': 2100, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:49:40,444]\u001b[0m Trial 2 finished with value: 0.758451449241466 and parameters: {'max_iter': 1900, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:49:52,560]\u001b[0m Trial 3 finished with value: 0.7295617913765453 and parameters: {'max_iter': 1800, 'dual': True}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:50:04,571]\u001b[0m Trial 4 finished with value: 0.7295617913765453 and parameters: {'max_iter': 2100, 'dual': True}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:50:19,255]\u001b[0m Trial 5 finished with value: 0.758451449241466 and parameters: {'max_iter': 1100, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:50:44,038]\u001b[0m Trial 6 finished with value: 0.758451449241466 and parameters: {'max_iter': 2600, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:51:02,901]\u001b[0m Trial 7 finished with value: 0.758451449241466 and parameters: {'max_iter': 2600, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:51:14,953]\u001b[0m Trial 8 finished with value: 0.7295617913765453 and parameters: {'max_iter': 1200, 'dual': True}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:51:26,177]\u001b[0m Trial 9 finished with value: 0.7295617913765453 and parameters: {'max_iter': 2900, 'dual': True}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:51:39,116]\u001b[0m Trial 10 finished with value: 0.758451449241466 and parameters: {'max_iter': 1600, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:51:50,732]\u001b[0m Trial 11 finished with value: 0.758451449241466 and parameters: {'max_iter': 2300, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:52:02,221]\u001b[0m Trial 12 finished with value: 0.758451449241466 and parameters: {'max_iter': 1600, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:52:13,492]\u001b[0m Trial 13 finished with value: 0.758451449241466 and parameters: {'max_iter': 1800, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:52:25,311]\u001b[0m Trial 14 finished with value: 0.758451449241466 and parameters: {'max_iter': 2300, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:52:37,853]\u001b[0m Trial 15 finished with value: 0.758451449241466 and parameters: {'max_iter': 1400, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:52:48,858]\u001b[0m Trial 16 finished with value: 0.758451449241466 and parameters: {'max_iter': 1900, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:53:01,085]\u001b[0m Trial 17 finished with value: 0.758451449241466 and parameters: {'max_iter': 2400, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:53:13,460]\u001b[0m Trial 18 finished with value: 0.758451449241466 and parameters: {'max_iter': 3000, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n",
      "\u001b[32m[I 2022-05-29 13:53:25,939]\u001b[0m Trial 19 finished with value: 0.758451449241466 and parameters: {'max_iter': 2900, 'dual': False}. Best is trial 1 with value: 0.758451449241466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 2100, 'dual': False}\n",
      "0.758451449241466\n",
      "CPU times: user 4min 6s, sys: 7.42 s, total: 4min 13s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    param = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 1000, 3000, 100),\n",
    "        'dual': trial.suggest_categorical('dual', [True, False]),\n",
    "    }\n",
    "\n",
    "    if param[\"dual\"] == True:\n",
    "        param[\"penalty\"] = 'l2'\n",
    "        param[\"loss\"] ='hinge'\n",
    "    elif param[\"dual\"] == False:\n",
    "        param[\"penalty\"] = 'l1'\n",
    "        param[\"loss\"] = 'squared_hinge'\n",
    "\n",
    "    cv_outer=StratifiedKFold(n_splits=5, random_state=12345, shuffle=True)\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('prep', TfidfVectorizer(stop_words=stopwords)),\n",
    "        ('est', LinearSVC(**param, random_state=12345))])       \n",
    "    \n",
    "    return cross_val_score(pipe, X_train, y_train, cv=cv_outer, scoring='f1').mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize', \n",
    "                            sampler = optuna.samplers.TPESampler(seed=0), \n",
    "                            study_name='Linear Support Vector Classification Optuna')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "#best parameters\n",
    "print(study.best_params)\n",
    "#best f1 score\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>The metric has improved! New best result is 0.7584. \n",
    "Now let's try the model on test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate tf-idf for train and test separately.\n",
    "#creating a counter\n",
    "\n",
    "count = TfidfVectorizer(stop_words = stopwords)\n",
    "count.fit_transform(X_train)\n",
    "tf_idf_train = count.transform(X_train)\n",
    "tf_idf_test = count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7502295684113865"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(penalty = 'l1', loss = 'squared_hinge', dual = False, random_state = 12345, max_iter = 2100)\n",
    "model.fit(tf_idf_train, y_train)\n",
    "predictions = predictions = model.predict(tf_idf_test)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<b>Data preparation</b>\n",
    "<<br>At the stage of data preparation, we cleared the texts of unnecessary words, lemmatized them and carried out vectorization using tf-idf. As data was big and heavy, we worked on a random sample of 50,000 texts. \n",
    "\n",
    "<b>Model training</b>\n",
    "<br> LinearSVC model showed the best results in f1 score and code execution time. The result of F1 is 0.74. RandomForest takes second place with f1 score equal to 0.73.\n",
    "<br>Using the selection of hyperparameters for LinearSVC, it was possible to achieve only a slight increase in f1 score = 0.7584. \n",
    "<br>The best model is <b>LinearSVC(penalty = 'l1', loss = 'squared_hinge', dual = False)</b> \n",
    "\n",
    "<br>We reached f1 equal to 0.7502 on the test data which meets the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
